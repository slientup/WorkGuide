## 从0开始学架构

### 一.基础架构
#### 架构选型评估维度
![架构设计评估维度](https://static001.geekbang.org/resource/image/b5/e3/b584ae29cc17bba9b7ad609e6ca2aae3.png)

这里缺少一个**开发周期的维度**

架构师经过思考后，给出了最终选择备选方案 2，原因有：
- 排除备选方案 1 的主要原因是可运维性，因为再成熟的系统，上线后都可能出问题，如果出问题无法快速解决，则无法满足业务的需求；并且 Kafka 的主要设计目标是高性能日志传输，
而我们的消息队列设计的主要目标是业务消息的可靠传输。
- 排除备选方案 3 的主要原因是复杂度，目前团队技术实力和人员规模（总共 6 人，还有其他中间件系统需要开发和维护）无法支撑自研存储系统（参考架构设计原则 2：简单原则）。
- 备选方案 2 的优点就是复杂度不高，也可以很好地融入现有运维体系，可靠性也有保障。

**选择方案1：**

案例很典型，所在项目，先选了3，1.0上线后效果不错，后期业务扩展，投入跟不上，3的缺点不断暴露，到后来大家就在吐槽为啥要造轮子。开始否决3，重构, 选择了1，运维话语权弱，被忽略了。
至于为啥不选2，就是面子上过不去，拿不出手。项目不光是为了业务，也为了架构师，领导的面子，被拿来和公司内其他项目做横向比较时，比较好吹。
至于运维的哥们，也乐意学些新东西，提升自我价值。所以，选择1大家都开心，除了项目的投入变大。

#### 细节设计
**1. 细化设计点 1：数据库表如何设计？**
- 数据库设计两类表，一类是**日志表**，用于消息写入时快速存储到 MySQL 中；另一类是消息表，**每个消息队列一张表**。
- 业务系统发布消息时，首先写入到日志表，日志表写入成功就代表消息写入成功；后台线程再从日志表中读取消息写入记录，将消息内容写入到消息表中。
- 业务系统读取消息时，从消息表中读取。
- 日志表表名为 MQ_LOG，包含的字段：日志 ID、发布者信息、发布时间、队列名称、消息内容。
- 消息表表名就是队列名称，包含的字段：消息 ID（递增生成）、消息内容、消息发布时间、消息发布者。
- 日志表需要及时清除已经写入消息表的日志数据，消息表最多保存 30 天的消息数据

采用**日志表**原因是：**尾部追加，性能高**

**2. 细化设计点 2：数据如何复制？**

直接采用 MySQL 主从复制即可，只复制消息存储表，不复制日志表。

**3. 细化设计点 3：主备服务器如何倒换？**
采用 ZooKeeper 来做主备决策，主备服务器都连接到 ZooKeeper 建立自己的节点，主服务器的路径规则为“/MQ/server/ 分区编号 /master”，备机为“/MQ/server/ 分区编号 /slave”，节点类型为 EPHEMERAL。

备机监听主机的节点消息，当发现主服务器节点断连后，备服务器修改自己的状态，对外提供消息读取服务。

**4. 细化设计点 4：业务服务器如何写入消息？**
- 消息队列系统设计两个角色：生产者和消费者，每个角色都有唯一的名称。
- 消息队列系统提供 SDK 供各业务系统调用，SDK 从配置中读取所有消息队列系统的服务器信息，SDK 采取轮询算法发起消息写入请求给主服务器。如果某个主服务器无响应或者返回错误，SDK 将发起请求发送到下一台服务器。

**5. 细化设计点 5：业务服务器如何读取消息**
- 消息队列系统提供 SDK 供各业务系统调用，SDK 从配置中读取所有消息队列系统的服务器信息，轮流向所有服务器发起消息读取请求。
- 消息队列服务器需要**记录每个消费者的消费状态**，即当前消费者已经读取到了哪条消息，当收到消息读取请求时，返回下一条未被读取的消息给消费者。

**6. 细化设计点 6：业务服务器和消息队列服务器之间的通信协议如何设计？**

**7.可以完善的细节**
1. 发送端和消费端如何寻址

   利用zookeeper做注册中心，把broker的地址注册到zk上，发送端和消费端只要配置注册中心的地址即可获取集群所以broker地址，当有broker下线时，发送端和消费端能及时更新broker地址。
2. 发送端消息重试

   当发送消息发生网络异常时（不包括超时异常），可以重新选择下一台broker来重试发送，重试策略可以自定义。
   
3. 消息消费采用pull还是push？

   考虑push模式会更复杂，故放弃，采用pull模式，消费端主动去拉，为了达到与push模式相同的低延迟效果，可以采用长轮询的方式，消费端轮询拉取消息费，当有消费可消费时，返回消息，如果没有可消费的消息，挂起当前线程，直到超时或者有可消费的消息为止。

4. 消息重复问题

   消息中间件不解决消息重复的问题，有业务系统自己根据业务的唯一id去重。
   
5. 顺序消息

   发送端在发生顺序消息时，只发送到相同broker的相同队列，消费端消费时，顺序消息只能由同一个消费端消息。
   
6. 定时消息
   
   发送端指定消息延时多长时间消费，**broker端定时扫描定时消息**，达到延时时间的消息加入到消费队列。
  
7. 事务消息

   发送端分两步，先预发送消息，broker端只记录消息为预发送状态，再执行本地事务，然后再根据本地事务的成功或者失败发送确认消息（回滚还是提交），这步如果发生异常，broker启动定时任务，把未确认的消息发送给发送端回查事务状态（需要发送端提供回查接口）
   
### 二.高性能架构模式

#### 高性能数据库集群：读写分离
读写分离的实现逻辑并不复杂，但有两个细节点将引入设计复杂度：**主从复制延迟**和**分配机制**

**主从复制延迟**

以 MySQL 为例，主从复制延迟可能达到 1 秒，如果有大量数据同步，延迟 1 分钟也是有可能的。

1. 写操作后的读操作指定发给数据库主服务器
   例如，注册账号完成后，登录时读取账号的读操作也发给数据库主服务器。这种方式和业务强绑定，对业务的侵入和影响较大，如果哪个新来的程序员不知道这样写代码，就会导致一个 bug。

2. 读从机失败后再读一次主机

   这就是通常所说的“二次读取”，二次读取和业务无绑定，只需要对底层数据库访问的 API 进行封装即可，实现代价较小，不足之处在于如果有很多二次读取，将大大增加主机的读操作压力。例如，黑客暴力破解账号，会导致大量的二次读取操作，主机可能顶不住读操作的压力从而崩溃。

3. 关键业务读写操作全部指向主机，非关键业务采用读写分离

   例如，对于一个用户管理系统来说，注册 + 登录的业务读写操作全部访问主机，用户的介绍、爱好、等级等业务，可以采用读写分离，因为即使用户改了自己的自我介绍，在查询时却看到了自我介绍还是旧的，业务影响与不能登录相比就小很多，还可以忍受。

**分配机制**
1. 程序代码封装(用户端端侧调用数据库选择jar)
2. 中间件封装(对程序来说中间件就是数据库)

**其他**

  我个人的想法是可以加入**缓存**，例如注册后登录这种业务，可以在注册后加入数据库，并加入缓存，登录的时候先查缓存再查库表。例如存入redis中并设置十分钟的过期时间。登录的时候先查redis，再查库表，如果redis中没有，说明就是过期的数据，这时候查从机就肯定存在了，希望能得到老师的点评，谢谢。
  
  **优化顺序：** SQL优化——缓存——读写分离——分库分表


#### 高性能数据库集群：分库分表

单台数据库服务器的存储能力会成为系统的瓶颈:
- 数据量太大，读写的性能会下降，即使有索引，索引也会变得很大，性能同样会下降。
- 数据文件会变得很大，数据库备份和恢复需要耗费很长时间。
- 数据文件越大，极端情况下丢失数据的风险越高（例如，机房火灾导致数据库主备机都发生故障）。
##### 业务分库
指的是按照**业务模块**将数据分散到不同的数据库服务器。例如，一个简单的电商网站，包括用户、商品、订单三个业务模块，我们可以将用户数据、商品数据、订单数据分开放到三台不同的数据库服务器上，而不是将所有数据都放在一台数据库服务器上

**带来问题：**
1. join 操作问题
  业务分库后，原本在同一个数据库中的表分散到不同数据库中，导致无法使用 SQL 的 join 查询
2. 事务问题
  原本在同一个数据库中不同的表可以在同一个事务中修改，业务分库后，表分散到不同的数据库中，无法通过事务统一修改。
  
##### 分表

单表数据拆分有两种方式：`垂直分表`和`水平分表`

`垂直分表`带来的问题：**操作的数量要增加**，原来只需要查询`一次`，现在却需要查询`两次`

`水平分表` 带来的问题：

- 路由  水平分表后，某条数据具体属于哪个切分后的子表，需要增加路由算法进行计算，这个算法会引入一定的复杂性

**范围路由：** 选取有序的数据列（例如，整形、时间戳等）作为路由的条件，不同分段分散到不同的数据库表中。以最常见的用户 ID 为例，路由算法可以按照 1000000 的范围大小进行分段，1 ~ 999999 放到数据库 1 的表中，1000000 ~ 1999999 放到数据库 2 的表中，以此类推

**Hash路由：** 选取某个列（或者某几个列组合也可以）的值进行 Hash 运算，然后根据 Hash 结果分散到不同的数据库表中

**配置路由：** 配置路由就是路由表，用一张独立的表来记录路由信息。同样以用户 ID 为例，我们新增一张 user_router 表，
这个表包含 user_id 和 table_id 两列，根据 user_id 就可以查询对应的 table_id

- count() 操作  记录数表：具体做法是新建一张表，假如表名为“记录数表”，包含 table_name、row_count 两个字段，每次插入或者删除子表数据成功后，都更新“记录数表
- order by 操作   水平分表后，数据分散到多个子表中，排序操作无法在数据库中完成，只能由业务代码或者数据库中间件分别查询每个子表中的数据，然后汇总进行排序

##### 实现方法
   和数据库读写分离类似，分库分表具体的实现方式也是“程序代码封装”和“中间件封装”，但实现会更复杂。读写分离实现时只要识别`SQL`操作是**读操作**还是**写操作**，通过简单的判断 SELECT、UPDATE、INSERT、DELETE 几个关键字就可以做到，而分库分表的实现除了**要判断操作类型**外，还要判断 SQL 中具体需要操作的表、操作函数（例如 count 函数)、order by、group by 操作等，然后再根据不同的操作进行不同的处理。例如 order by 操作，需要先从多个库查询到各个库的数据，然后再重新 order by 才能得到最终的结果.


#### 高性能NoSQL
关系数据库存在如下缺点：

-  关系数据库存储的是行记录，无法存储数据结构

   以微博的关注关系为例，“我关注的人”是一个用户 ID 列表，使用关系数据库存储只能将列表拆成多行，然后再查询出来组装，无法直接存储一个列表。
   
-  关系数据库的`schema`扩展很不方便

   关系数据库的表结构 schema 是强约束，操作不存在的列会报错，业务变化时扩充列也比较麻烦，需要执行 DDL（data definition language，如 CREATE、ALTER、DROP 等）语句修改，而且修改时可能会长时间锁表（例如，MySQL 可能将表锁住 1 个小时）

- 关系数据库在大数据场景下 I/O 较高
  
  如果对一些大量数据的表进行统计之类的运算，关系数据库的 I/O 会很高，因为即使**只针对其中某一列进行运算，关系数据库也会将整行数据从存储设备读入内存**.
 
- 关系数据库的全文搜索功能比较弱

  关系数据库的全文搜索只能使用 like 进行整表扫描匹配，性能非常低，在互联网这种搜索复杂的场景下无法满足业务要求

针对上述问题，分别诞生了不同的 NoSQL 解决方案，这些方案与关系数据库相比，在某些应用场景下表现更好。但世上没有免费的午餐，NoSQL 方案带来的优势，
本质上是牺牲`ACID`中的**某个或者某几个特性**

因此我们不能盲目地迷信`NoSQL`是银弹，而应该将 NoSQL 作为 SQL 的一个有力补充，NoSQL != No SQL，而是 **NoSQL = Not Only SQL**

**常见的 NoSQL 方案分为 4 类**
- K-V 存储：解决关系数据库无法存储数据结构的问题，以 `Redis` 为代表。
- 文档数据库：解决关系数据库强 schema 约束的问题，以 `MongoDB` 为代表。
- 列式数据库：解决关系数据库大数据场景下的 I/O 问题，以 `HBase`为代表。
- 全文搜索引擎：解决关系数据库的全文搜索性能问题，以 `Elasticsearch` 为代表

`redis`主要解决结构性数据问题，缺点主要体现在并不支持完整的 ACID 事务，Redis 虽然提供事务功能，但 Redis 的事务和关系数据库的事务不可同日而语，Redis 的事务只能保证`隔离性`和`一致性（I 和 C），无法保证原子性和持久性（A 和 D）`
`mongodb`主要解决schema的问题，有以下特点：1. 新增字段简单，无需做任何操作，2. 历史数据不会出错  3. 可以很容易存储复杂数据(json数据类型)

缺点是：最主要的代价就是不支持事务，无法实现关系数据库的`join`操作。

`列式数据库` 行式存储即使最终只使用一列，也会将所有行数据都读取出来,而列式存储就只会读取**一列**数据 但当需要频繁地更新多个列。因为列式存储将不同列存储在磁盘上不连续的空间，导致更新多个列时磁盘是随机写操作；而行式存储时同一行多个列都存储在连续的空间，一次磁盘写操作就可以完成，列式存储的随机**写效率**要远远低于行式存储的写效率,所以适用的场景是大数据离线分析

`全文搜索引擎` 

传统数据库面临的缺点；
- 全文搜索的条件可以随意排列组合，如果通过索引来满足，则索引的数量会非常多。 
- 全文搜索的模糊匹配方式，索引无法满足，只能用 like 查询，而`like`查询是整表扫描，效率非常低。

全文搜索基本原理：

全文搜索引擎的技术原理被称为“倒排索引”(Inverted index),其基本原理是建立**单词**到**文档**的索引

**倒排索引示例**
![](https://static001.geekbang.org/resource/image/3c/bd/3cfa4abdcc22015ade669d9a844ae1bd.jpg)

**数据库的选择**

关于NoSQL，看过一张图，挺形象：“1970，We have no SQL”-“1980，Know SQL”-“2000，NoSQL”-“2005，Not only SQL”-“2015，No，SQL”。
目前，一些新型数据库，同时具备了NoSQL的扩展性和关系型数据库的很多特性。

关系型和NoSQL数据库的选型。考虑几个指标，数据量、并发量、实时性、一致性要求、读写分布和类型、安全性、运维性等。根据这些指标，软件系统可分成几类。
1. 管理型系统，如运营类系统，首选关系型。
2. 大流量系统，如电商单品页的某个服务，后台选关系型，前台选内存型。
3. 日志型系统，原始数据选列式，日志搜索选倒排索引。
4. 搜索型系统，指站内搜索，非通用搜索，如商品搜索，后台选关系型，前台选倒排索引。
5. 事务型系统，如库存、交易、记账，选关系型+缓存+一致性协议，或新型关系数据库。
6. 离线计算，如大量数据分析，首选列式，关系型也可以。
7. 实时计算，如实时监控，可以选时序数据库，或列式数据库。

####  高性能缓存架构
在某些复杂的业务场景下，单纯依靠存储系统的性能提升不够的，还需要借助**缓存**

**缓存适用场景**

- 需要经过复杂运算后得出的数据，存储系统无能为力
例如，一个论坛需要在首页展示当前有多少用户同时在线，如果使用 MySQL 来存储当前用户状态，则每次获取这个总数都要“count(*)”大量数据，这样的操作无论怎么优化 MySQL，性能都不会太高。如果要实时展示用户同时在线数，则 MySQL 性能无法支撑

- 读多写少的数据，存储系统有心无力
绝大部分在线业务都是读多写少。例如，微博、淘宝、微信这类互联网业务，读业务占了整体业务量的`90%`以上。以微博为例：一个明星发一条微博，可能几千万人来浏览。如果使用`MySQL`来存储微博，用户写微博只有一条 `insert`语句，但每个用户浏览时都要`elect`一次，即使有索引，几千万条 select 语句对 MySQL 数据库的压力也会非常大.

**缓存可能的问题**

1. **缓存穿透** 业务系统虽然去缓存查询数据，但缓存中没有数据，业务系统需要再次去存储系统查询数据
- 存储数据不存在 
- 缓存数据生成耗费大量时间或者资源 
2. **缓存雪崩** 当缓存失效（过期）后引起系统性能急剧下降的情况 当缓存过期被清除后，业务系统需要重新生成缓存，因此需要再次访问存储系统，再次进行运算，这个处理步骤耗时几十毫秒甚至上百毫秒

   解决方法：

   - **更新锁**  对缓存更新操作进行加锁保护，保证只有一个线程能够进行缓存更新，未能获取更新锁的线程要么等待锁释放后重新读取缓存，要么就返回空值或者默认值
   - **后台更新** 由后台线程来更新缓存，而不是由业务线程来更新缓存，缓存本身的有效期设置为永久，**后台线程定时更新缓存**  后台定时机制需要考虑一种特殊的场景，**当缓存系统内存不够时**，会“踢掉”一些缓存数据，从缓存被“踢掉”到下一次定时更新缓存的这段时间内，业务线程读取缓存返回空值，而业务线程本身又不会去更新缓存，因此业务上看到的现象就是数据丢了。
   **解决思路：**
   
   业务线程发现缓存失效后，通过**消息队列**发送一条消息通知后台线程更新缓存。可能会出现多个业务线程都发送了缓存更新消息，但其实对后台线程没有影响，后台线程收到消息后更新缓存前可以判断缓存是否存在，存在就不执行更新操作。这种方式实现依赖消息队列，复杂度会高一些，但缓存更新更及时，用户体验更好
   
3. **缓存热点**  缓存中会存在特别热点的数据，这类数据的频繁访问会给缓存服务器带来压力。

   解决方案：缓存副本，分布到不同的机器 **缓存热点的解决方案就是复制多份缓存副本，将请求分散到多个缓存服务器上，减轻缓存热点导致的单台缓存服务器压力**，缓存副本设计有一个细节需要注意，就是不同的缓存副本不要设置统一的过期时间，否则就会出现所有缓存副本同时生成同时失效的情况，从而引发缓存雪崩效应

数据库自身不是有缓存吗，标准答案是怎么回击他?
以下仅限mysql：
1. mysql第一种缓存叫sql语句结果缓存，但条件比较苛刻，程序员不可控，我们的dba线上都关闭这个功能，具体实现可以查一下
2. mysql第二种缓存是innodb buffer pool，缓存的是磁盘上的分页数据，不是sql的查询结果，sql的执行过程省不了。而mc，redis这些实际上都是缓存sql的结果，两种缓存方式，性能差很远。

因此，可控性，性能是数据库缓存和独立缓存的主要区别

#### 单服务器高性能架构
- 尽量提升单服务器的性能，将单服务器的性能发挥到极致。
- 如果单服务器无法支撑性能，设计服务器集群方案。

单服务器高性能的关键之一就是服务器采取的并发模型，并发模型有如下两个关键设计点：
- 服务器如何管理连接。
- 服务器如何处理请求。

PPC最大的问题是每个连接都要创建进程，这样极度浪费资源，所以，自然而然的方案是资源复用，创建对应的进程池.这里引出一个新的问题，进程如何才能高效地处理**多个连接的业务？**

`read操作`改为非阻塞，然后进程不断地轮询多个连接。这种方式能够解决阻塞的问题，但解决的方式并不优雅。首先，轮询是要消耗`CPU`的；
其次，如果一个进程处理几千上万的连接，则轮询的效率是很低的

解决方法就是：**只有当连接上有数据的时候进程**才去处理，这就是 `I/O 多路复用技术的来源`
- 当多条连接共用一个阻塞对象后，进程只需要在一个阻塞对象上等待，而无须再轮询所有连接，常见的实现方式有 select、`epoll`、kqueue 等。
- 当某条连接有新的数据可以处理时，**操作系统会通知进程**，进程从阻塞状态返回，**开始进行业务处理。**

`Reactor`模式的核心组成部分包括 `Reactor`和处理资源池（`进程池或线程池`），其中`Reactor`负责监听和分配事件，处理资源池负责**处理事件**

单 Reactor 多线程方案示意图是：
![](https://static001.geekbang.org/resource/image/7c/43/7c299316e48b0531328ba39261d1d443.png)
- 主线程中，Reactor 对象通过 select 监控连接事件，收到事件后通过 dispatch 进行分发。
- 如果是连接建立的事件，则由 Acceptor 处理，Acceptor 通过 accept 接受连接，并创建一个 Handler 来处理连接后续的各种事件。
- 如果不是连接建立事件，则 Reactor 会调用连接对应的 Handler（第 2 步中创建的 Handler）来进行响应。
- Handler 只负责响应事件，不进行业务处理；Handler 通过 read 读取到数据后，会发给 Processor 进行业务处理。
- Processor 会在独立的子线程中完成真正的业务处理，然后将响应结果发给主进程的 Handler 处理；Handler 收到响应后通过 send 将响应结果返回给 client。


目前 Windows 下通过 IOCP 实现了真正的异步 I/O，而在 Linux 系统下的 AIO 并不完善，因此在 Linux 下实现高并发网络编程时都是以 Reactor 模式为主。所以即使 Boost.Asio 号称实现了 Proactor 模型，其实它在 Windows 下采用 IOCP，而在 Linux 下是用 Reactor 模式（采用 epoll）模拟出来的异步模型

内核接收网络数据全过程
![](https://img-blog.csdnimg.cn/20191025213749309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FybWxpbnV4d3c=,size_16,color_FFFFFF,t_70)

[IO多路复用技术](https://www.zhihu.com/question/28594409)
[epoll原理解析](https://blog.csdn.net/armlinuxww/article/details/92803381)


#### 高性能负载均衡：分类及架构
单服务器无论如何优化，无论采用多好的硬件，总会有一个性能天花板，当单服务器的性能无法满足业务需求时，就需要设计高性能集群来提升系统整体的处理性能,高性能集群的本质很简单，通过增加更多的服务器来提升系统整体的计算能力

常见的负载均衡系统包括 3 种：**DNS负载均衡**、**硬件负载均衡**和**软件负载均衡**

`DNS负载均衡`用于实现地理级别的负载均衡；`硬件负载均衡`用于实现集群级别的负载均衡；`软件负载均衡`用于实现**机器级别**的负载均衡

日活千万的论坛，这个流量不低了。
1、首先，流量评估。1000万DAU，换算成秒级，平均约等于116。考虑每个用户操作次数，假定10，换算成平均QPS=1160。考虑峰值是均值倍数，假定10，换算成峰值QPS=11600。
  考虑静态资源、图片资源、服务拆分等，流量放大效应，假定10，QPS*10=116000
  
2、其次，容量规划。考虑高可用、异地多活，QPS*2=232000。考虑未来半年增长，QPS*1.5=348000

3、最后，方案设计。三级导流。第一级，DNS，确定机房，以目前量级，可以不考虑。第二级，确定集群，扩展优先，则选Haproxy/LVS，稳定优先则选F5。第三级，Nginx+KeepAlived，确定实例。

### 高可用架构模式
在一个分布式系统（**指互相连接并共享数据的节点的集合**）中，当涉及读写操作时，只能保证一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三者中的两个，另外一个必须被牺牲

**CAP关注的范围：**

CAP关注的是对**数据的读写操作，而不是分布式系统的所有功能**，且只讨论会**互相连接并共享数据的节点的集合**。

虽然 CAP 理论定义是三个要素中只能取两个，但放到分布式环境下来思考，我们会发现必须选择 P（分区容忍）要素，因为网络本身无法做到 100% 可靠，有可能出故障，所以分区是一个必然的现象。如果我们选择了 CA 而放弃了 P，那么当发生分区现象时，为了保证 C，系统需要禁止写入，当有写入请求时，系统返回 error（例如，当前系统不允许写入），这又和 A 冲突了，因为 A 要求返回 no error 和 no timeout。因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。

**CAP 关键细节点**

- CAP 关注的粒度是**数据**，而不是整个系统
   但这句话是理解和应用 CAP 理论非常关键的一点。CAP 理论的定义和解释中，用的都是 system、node 这类系统级的概念，这就给很多人造成了很大的误导，认为我们在进行架构设计时，整个系统要么选择 CP，要么选择 AP。但在实际设计过程中，每个系统不可能只处理一种数据，而是包含多种类型的数据，有的数据必须选择 CP，有的数据必须选择 AP

- `CAP`是忽略网络延迟的
   这是一个非常隐含的假设，布鲁尔在定义一致性时，并没有将延迟考虑进去。也就是说，当事务提交时，数据能够瞬间复制到所有节点

- 正常运行情况下，不存在 CP 和 AP 的选择，可以同时满足 CA
  既要考虑分区发生时选择 CP 还是 AP，也要考虑分区没有发生时如何保证 CA

- 放弃并不等于什么都不做，需要为分区恢复后做准备


### 可扩展架构模式
软件系统与硬件最大的区别就是软件不是一层不变，它随时都在变化，软件系统的这种天生和内在的可扩展的特性，既是魅力所在，又是难点所在。魅力体现在我们可以通过修改和扩展，不断地让软件系统具备更多的功能和特性，满足新的需求或者顺应技术发展的趋势。而难点体现在如何以**最小的代价去扩展系统**  

可扩展架构对象是：**软件系统**

**核心思想 ：拆**   目的在于降低改动范围

按照不同的思路来拆分软件系统，就会得到不同的架构。常见的拆分思路有如下三种

- **面向流程拆分：** 将整个业务流程拆分为几个阶段，每个阶段作为一部分。
- **面向服务拆分：** 将系统提供的服务拆分，每个服务作为一部分。
- **面向功能拆分：** 将系统提供的功能拆分，每个功能作为一部分

如何理解“流程”“服务”“功能”三者的联系和区别  从范围上来看，从大到小依次为：**流程 > 服务 > 功能**

不同的拆分方式，将得到不同的系统架构，典型的可扩展系统架构有：
- 面向流程拆分：**分层架构**
- 面向服务拆分：**SOA、微服务**
- 面向功能拆分：**微内核架构**

SOA是把多个系统整合，而微服务是把单个系统拆开来，方向正好相反
  

