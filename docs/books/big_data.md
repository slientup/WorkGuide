# 极客时间 从0开始学大数据

### 大数据生态

![大数据生态](https://static001.geekbang.org/resource/image/ca/73/ca6efc15ead7fb974caaa2478700f873.png)

### 大数据应用的发展 

从数据搜索——数据仓库时代——数据挖掘时代——机器学习时代  无论是哪一个时代都是**发现数据中的规律并为我们提供价值**，在最开始的搜索引擎时代，是对每条网页的分词做统计，
数据仓库时代需要从海量的数据中获取需要的数据(如做报表统计等)，而数据挖掘时代实际上想做数据与数据之间的关联性分析，而机器学习时代，是对某个场景的结果做统计分析，，选择其中的最优解。无论是处在哪个时代，赋予数据灵魂的始终是人的智慧，是人决定了如何利用这海量的数据，是人的智慧决定了大数据应用的上限。

## hadoop大数据原理和架构

### 移动计算比移动数据更划算

传统软件计算处理模型，都是`输入---计算---输出`模型

但在大数据领域，常常需要处理的数据时以PB为单位的，如果一个程序需要输入PB的数据量，然后进行处理，这个程序会崩溃的，而大数据解决的思想就是分布式，分而治之

大数据计算处理通常针对的是网站的存量数据，将其中规律统计出来后进行改善以提升服务质量

我们在分布式的场景下，应该是移动数据还是移动计算程序啦？答案是移动计算，因为计算程序往往很小，而数据却会很大，移动数据的成本会大得多。

### 从RAID看垂直伸缩到水平伸缩的演化

如果一个文件的大小超过了一张磁盘的大小，你该如何存储？

单机时代，主要的解决方案是`RAID`；分布式时代，主要解决方案是分布式文件系统

存储系统要解决如下问题？
- 存储容量，如何存储大规模的文件？
- 读写速度，单个磁盘的读写速度一般在几十兆，在数据是以PB为单位的大数据领域，如何快速读取？
- 数据的备份 其中一块磁盘异常了 数据不会丢失

`raid` 是将多个磁盘组成一个阵列，统一对外提供服务，改善了存储容量、读写速度，增强磁盘的可用性和容错能力。

几种常用raid
![](https://static001.geekbang.org/resource/image/54/af/54e170b7438fe3b8f8196dbfbc943baf.jpg)
raid效率比较
![](https://static001.geekbang.org/resource/image/e2/2f/e2fb7ec97e6127c1b03e83daeff0232f.jpg)

`raid`是一种垂直扩展的思路，即在单机条件下做相应的冗余可靠性，但是单机无论有多强都是有极限的，而且成本会越来越高，相反，如果采用分布式，将垂直的方式
改成`水平伸缩`的模式就能实现无限的扩张，实现更强的计算能力。

RAID 技术只是在单台服务器的多块磁盘上组成阵列，大数据需要更大规模的存储空间和更快的访问速度。将 RAID 思想原理应用到分布式服务器集群上，就形成了 Hadoop 分布式文件系统 HDFS 的架构思想

连续写入：写入只寻址一次 存储位置与逻辑位置相邻 不用多次寻址

随机写入：每写一次 便寻址一次 增加了磁盘的寻址时间

总结：垂直伸缩总有尽头，水平伸缩理论上是没有止境的

### 新技术层出不穷，HDFS依然是存储的王者

从大数据发展至今，计算框架，应用场景等都在不断的变化，唯一不变的就是存储系统`HDFS`,因为大数据的核心在**数据**

为什么 HDFS 的地位如此稳固呢？

在整个大数据体系里面，最宝贵、最难以代替的资产就是数据，**大数据所有的一切都要围绕数据展开**。HDFS 作为最早的大数据存储系统，存储着宝贵的数据资产，各种新的算法、框架要想得到人们的广泛使用，必须支持 HDFS 才能获取已经存储在里面的数据。所以大数据技术越发展，新技术越多，HDFS 得到的支持越多，我们越离不开 HDFS。**HDFS 也许不是最好的大数据存储技术，但依然最重要的大数据存储技术**

Hadoop 分布式文件系统 HDFS 的设计目标是管理数以千计的服务器、数以万计的磁盘，将这么大规模的服务器计算资源当作一个单一的存储系统进行管理，对应用程序提供数以 PB 计的存储容量，让应用程序像使用普通文件系统一样存储大规模的文件数据。

我们在设计一个产品的时候，要解决用户的问题，也要对用户无感知，不改变它的使用习惯。

**HDFS架构图**
![](https://static001.geekbang.org/resource/image/65/d7/65efd126cbcf3930a706f64c6e6457d7.jpg)

**DataNode** 负责文件数据的存储和读写操作，HDFS 将文件数据分割成若干数据块（`Block`），每个 DataNode 存储一部分数据块，这样文件就分布存储在整个 HDFS 服务器集群中  **数据块**是基本单位

**NameNode** 负责整个分布式文件系统的元数据（`MetaData`）管理，也就是文件路径名、数据块的 ID 以及存储位置等信息，相当于操作系统中文件分配表（FAT）的角色 HDFS 为了保证数据的高可用，会将一个数据块复制为多份（缺省情况为 3 份），并将多份相同的数据块存储在不同的服务器上，甚至不同的机架上

**数据块多份复制存储的示意图**
![](https://static001.geekbang.org/resource/image/6f/ac/6f2faa48524251ad77e55e3565095bac.jpg)
图中对于文件 /users/sameerp/data/part-0，其复制备份数设置为 2，存储的 BlockID 分别为 1、3。Block1 的两个备份存储在 DataNode0 和 DataNode2 两个服务器上，Block3 的两个备份存储 DataNode4 和 DataNode6 两个服务器上，上述任何一台服务器宕机后，每个数据块都至少还有一个备份存在，不会影响对文件 /users/sameerp/data/part-0 的访问

**HDFS的不同层面的高可用设计**
1. 数据存储故障容错  通过`校验和`的方式

磁盘介质在存储过程中受环境或者老化影响，其存储的数据可能会出现错乱。HDFS 的应对措施是，对于存储在 DataNode 上的数据块，计算并存储校验和（CheckSum）。在读取数据的时候，重新计算读取出来的数据的校验和，如果校验不正确就抛出异常，应用程序捕获异常后就到其他 DataNode 上读取备份数据。

2. 磁盘故障容错  datanode监控磁盘状态并上报给namenode

如果 DataNode 监测到本机的某块磁盘损坏，就将该块磁盘上存储的所有 BlockID 报告给 NameNode，NameNode 检查这些数据块还在哪些 DataNode 上有备份，通知相应的 DataNode 服务器将对应的数据块复制到其他服务器上，以保证数据块的备份数满足要求

3. datanode故障容错 datanode和namenode的心跳机制

DataNode 会通过心跳和 NameNode 保持通信，如果 DataNode 超时未发送心跳，NameNode 就会认为这个 DataNode 已经宕机失效，立即查找这个 DataNode 上存储的数据块有哪些，以及这些数据块还存储在哪些服务器上，随后通知这些服务器再复制一份数据块到其他服务器上，保证 HDFS 存储的数据块备份数符合用户设置的数目，即使再出现服务器宕机，也不会丢失数据

4. NameNode 故障容错 

namenode是整个机器的核心，一定不能故障 主备机制 用zookeeper做znode锁
![](https://static001.geekbang.org/resource/image/7c/89/7cb2668644c32364beab0b69e60b3689.png)

可用性是在网站架构设计中必须考虑的一个因素，因为我们随时都在面对着意外的发生，内存、CPU、主板、磁盘会损坏，服务器会宕机，网络会中断，机房会停电等

**保证系统的可用性一般有三种方法：冗余备份，失效转移，降级限流**

比如冗余备份，任何程序、任何数据，都至少要有一个备份，也就是说程序至少要部署到两台服务器，数据至少要备份到另一台服务器上。此外，稍有规模的互联网企业都会建设多个数据中心，数据中心之间互相进行备份，用户请求可能会被分发到任何一个数据中心，即所谓的异地多活，在遭遇地域性的重大故障和自然灾害的时候，依然保证应用的高可用

当要访问的程序或者数据无法访问时，需要将访问请求转移到备份的程序或者数据所在的服务器上，这也就是**失效转移**。失效转移你应该注意的是失效的鉴定

当大量的用户请求或者数据处理请求到达的时候，由于计算资源有限，可能无法处理如此大量的请求，进而导致资源耗尽，系统崩溃。这种情况下，可以拒绝部分请求，即进行限流；也可以关闭部分功能，降低资源消耗，即进行降级

### 为什么说MapReduce既是编程模型又是计算框架？

MapReduce 既是一个`编程模型`，又是一个`计算框架` 开发人员必须基于 MapReduce 编程模型进行编程开发，然后将程序通过 MapReduce 计算框架分发到 Hadoop 集群中运行


假如计算单词出现的频率 对应的MapReduce程序如下：
```java
public class WordCount {

  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }
}
```
mapreduce的计算过程大概如下：
![](https://static001.geekbang.org/resource/image/55/ba/5571ed29c5c2254520052adceadf9cba.png)
还需要一个计算框架，能够调度执行这个 MapReduce 程序，使它在分布式的集群中并行运行，而这个计算框架也叫 `MapReduce`

模型是人们对一类事物的概括与抽象，可以帮助我们更好地理解事物的本质，更方便地解决问题。比如，数学公式是我们对物理与数学规律的抽象，地图和沙盘是我们对地理空间的抽象，软件架构图是软件工程师对软件系统的抽象

### MapReduce如何让数据完成一次旅行？
上面的MapReduce程序如何在分布式集群中运行起来啦？MapReduce程序如何找到对应的数据并进行计算的啦？这个就需要MapReduce计算框架来实现了

框架处理位置
![](https://static001.geekbang.org/resource/image/f3/9c/f3a2faf9327fe3f086ec2c7eb4cd229c.png)

**MapReduce 的启动和运行机制**
![](https://static001.geekbang.org/resource/image/2d/27/2df4e1976fd8a6ac4a46047d85261027.png)

如果我们把这个计算过程看作一次小小的旅行，这个旅程可以概括如下:

1. 应用进程 JobClient 将用户作业 JAR 包存储在 HDFS 中，将来这些 JAR 包会分发给 Hadoop 集群中的服务器执行 MapReduce 计算。

2. 应用程序提交 job 作业给 JobTracker。

3.JobTracker 根据作业调度策略创建 JobInProcess 树，每个作业都会有一个自己的 JobInProcess 树。

4.JobInProcess 根据输入数据分片数目（通常情况就是数据块的数目）和设置的 Reduce 数目创建相应数量的 TaskInProcess。

5.TaskTracker 进程和 JobTracker 进程进行定时通信。

6. 如果 TaskTracker 有空闲的计算资源（有空闲 CPU 核心），JobTracker 就会给它分配任务。分配任务的时候会根据 TaskTracker 的服务器名字匹配在同一台机器上的数据块计算任务给它，使启动的计算任务正好处理本机上的数据，以实现我们一开始就提到的“移动计算比移动数据更划算”。

7.TaskTracker 收到任务后根据任务类型（是 Map 还是 Reduce）和任务参数（作业 JAR 包路径、输入数据文件路径、要处理的数据在文件中的起始位置和偏移量、数据块多个备份的 DataNode 主机名等），启动相应的 Map 或者 Reduce 进程。

8.Map 或者 Reduce 进程启动后，检查本地是否有要执行任务的 JAR 包文件，如果没有，就去 HDFS 上下载，然后加载 Map 或者 Reduce 代码开始执行。

9. 如果是 Map 进程，从 HDFS 读取数据（通常要读取的数据块正好存储在本机）；如果是 Reduce 进程，将结果数据写出到 HDFS。

**MapReduce 计算真正产生奇迹的地方是数据的合并与连接**

MapReduce 是如何让相同的`key`发送到同一个reduce进程处理的啦？ 
在`Map`计算结束后，`MapReduce`通过hash的方式调用map结果，然后保证同一个key的信息是同一个`reduce`处理 如下是处理图：
![](https://static001.geekbang.org/resource/image/d6/c7/d64daa9a621c1d423d4a1c13054396c7.png)

每个 Map 任务的计算结果都会写入到**本地文件系统**，等`Map`任务快要计算完成的时候，MapReduce 计算框架会启动 `shuffle` 过程，在 Map 任务进程调用一个 Partitioner 接口，对 Map 产生的每个 <Key, Value> 进行 Reduce 分区选择，然后通过 `HTTP`通信发送给对应的`Reduce`进程。这样不管 Map 位于哪个服务器节点，相同的 Key 一定会被发送给相同的 Reduce 进程。Reduce 任务进程对收到的 <Key, Value> 进行排序和合并，相同的 Key 放在一起，组成一个 <Key, Value 集合 > 传递给 Reduce 执行

**分布式计算需要将不同服务器上的相关数据合并到一起进行下一步计算，这就是 shuffle**

为什么mapper计算完的结果要放到硬盘呢？那再发送到reducer不是还有个读取再发送的过程吗？这中间不就有一个重复的写和读的过程吗？

**是的，主要为了可靠性，spark就不写硬盘，所以快**

当某个key聚集了大量数据，shuffle到同一个reduce来汇总，考虑数据量很大的情况，这个会不会把reduce所在机器节点撑爆？这样任务是不是就失败了？

会的，数据倾斜，会导致任务失败。严重的数据倾斜可能是数据本身的问题，需要做好预处理

移动计算主要是map阶段，reduce阶段数据还是要移动数据合并关联，不然很多计算无法完成

### 为什么我们管Yarn叫作资源调度框架？
资源调度框架，它会根据每台机器此刻的运行指标来对任务进行调度，将任务分发到最合适的机器上

MapReduce架构存在一个问题：服务器集群资源调度管理和 MapReduce 执行过程耦合在一起，如果想在当前集群中运行其他计算任务，比如 Spark 或者 Storm，就无法统一使用集群中的资源了，但随着大数据技术的发展，各种新的计算框架不断出现，越是我们就需要解耦，将Yarn从MapReduce中分离出来，这就是Hadoop 2 最主要的变化

**Yarn架构**
![](https://static001.geekbang.org/resource/image/af/b1/af90905013e5869f598c163c09d718b1.jpg)
Yarn由两部分组成 1.资源管理器 2.节点管理器 ,`ResourceManager` 进程负责整个集群的资源调度管理，通常部署在独立的服务器上；`NodeManager` 进程负责具体服务器上的资源和任务管理，在集群的每一台计算服务器上都会启动，基本上跟 HDFS 的 DataNode 进程一起出现

资源管理器又包括两个主要组件：调度器和应用程序管理器

调度器其实就是一个资源分配算法，根据应用程序（Client）提交的资源申请和当前服务器集群的资源状况进行资源分配

应用程序管理器负责应用程序的提交、监控应用程序运行状态等 应用程序启动后需要在集群中运行一个`ApplicationMaster`，`ApplicationMaster`也需要运行在容器里面

以一个 `MapReduce`程序为例，来看一下 `Yarn` 的整个工作流程。
1. 我们向 Yarn 提交应用程序，包括 MapReduce ApplicationMaster、我们的 MapReduce 程序，以及 MapReduce Application 启动命令。

2.ResourceManager 进程和 NodeManager 进程通信，根据集群资源，为用户程序分配第一个容器，并将 MapReduce ApplicationMaster 分发到这个容器上面，并在容器里面启动 MapReduce ApplicationMaster。

3.MapReduce ApplicationMaster 启动后立即向 ResourceManager 进程注册，并为自己的应用程序申请容器资源。

4.MapReduce ApplicationMaster 申请到需要的容器后，立即和相应的 NodeManager 进程通信，将用户 MapReduce 程序分发到 NodeManager 进程所在服务器，并在容器中运行，运行的就是 Map 或者 Reduce 任务。

5.Map 或者 Reduce 任务在运行期和 MapReduce ApplicationMaster 通信，汇报自己的运行状态，如果运行结束，MapReduce ApplicationMaster 向 ResourceManager 进程注销并释放所有的容器资源。

为什么 HDFS 是系统，而 MapReduce 和 Yarn 则是框架？

框架在设计的时候有一个重要思想："依赖倒转原则" 即高层模块不能依赖底层模块，他们应该共同依赖一个抽象，这个抽象由**高层模块**定义，由底层模块实现

所谓高层模块和低层模块的划分，简单说来就是在调用链上，处于前面的是高层，后面的是低层

HDFS 就不是框架，使用 HDFS 就是直接调用 HDFS 提供的 API 接口，HDFS 作为底层模块被直接依赖

是的，但是更重要的是接口是高层需求的抽象，还是底层实现的抽象。这是依赖倒置的关键，面向接口本身并不能保证依赖倒置原则，否则和接口隔离原则没有区别

### 我们能从Hadoop学到什么？

大数据因为要对数据和计算任务进行统一管理，所以和互联网在线应用不同，需要一个全局管理者。而在线应用因为每个用户请求都是独立的，而且为了高性能和便于集群伸缩，会尽量避免有全局管理者 

学习新知识:遵循一个**5-20-2 法则** 用 5 分钟的时间了解这个新知识的特点、应用场景、要解决的问题；用 20 分钟理解它的主要设计原理、核心思想和思路；再花 2 个小时看关键的设计细节，尝试使用或者做一个 demo 

如果 5 分钟不能搞懂它要解决的问题，我就会放弃；20 分钟没有理解它的设计思路，我也会放弃；2 个小时还上不了手，我也会放一放。你相信我，一种真正有价值的好技术，你这次放弃了，它过一阵子还会换一种方式继续出现在你面前。这个时候，你再尝试用 5-20-2 法则去学习它，也许就会能理解了。我学 Hadoop 实际上就是经历了好几次这样的过程，才终于入门。而有些技术，当时我放弃了，它们再也没有出现在我面前，后来它们被历史淘汰了，我也没有浪费自己的时间.

大数据的思考：

我认为，学习一个新的东西，首先要弄清楚三件事：这是什么东西（干什么的）？为什么需要它（怎么来的）？它是如何运作的？

随着互联网信息产业的发展，网络上时刻产生的数据以及沉淀的历史数据量规模呈爆炸式增长，而计算机硬件诸如CPU、内存等性能的增长速度远远跟不上数据的增长速度，因此传统的单机处理程序已经无法满足数据的处理需求。分布式处理系统应运而生，这是大数据系统的前身。

大数据系统主要处理大规模（一般指PB级别的数据量）的动态和存量数据，通过大量数据的读取和分析，能够从中找出人们从未关注到的甚至没有想到的 事物之间的关联关系，并以此为人们日常生活以及各种生产活动提供必需的决策支持。

对于大数据系统的运作原理，我想从一个设计者的角度来思考：
要清晰的知道，不论大数据系统功能多么强大，性能多么NB，体系如何庞杂，它本质上依然和传统软件的运作模型是一样的：I-P-O，没错，就是输入-计算-输出。以MapReduce为例，输入就是各taskStracker在本地各自读取数据分片；而计算过程有2大的步骤：1是map进程阶段将原始数据进行初步合并计算，并将得出的结果发给reduce进程，我把这个过程称为预处理，预处理后的数据量会降低到网络可以承受的地步；2是reduce收到map传来的预处理数据，并进行最终合并计算；输出部分:reduce进程将最终计算的结果保存到HDFS（本地数据块），并由HDFS将所有reduce保存的数据块合并成一个HDFS文件。你看，这个过程是不是就是一个IPO过程？只不过每个具体过程的执行者以及数量发生了改变。

OK，弄清楚了基本运作模型，接下来就是考虑：针对大规模数据分散存储（先不考虑实时数据哈）在大量服务器上的数据存储背景，如何设计一款软件，能够高效读取、处理这些数据，并有效输出呢？

首先是数据读写。现在大家都清楚，在数据规模达到PB级别的情况下，如果使用集中读取，集中处理的方式，单机的硬件和网络根本承载不了。所以最好的办法就是让数据所在的服务器自行读取本地数据并进行计算，然后将每个服务器计算结果汇总后在写入本地，当然所有服务器写入本地的数据最终又会汇总成为一个可以被识别的输出文件。那么如何让每台服务都知道自己应该读哪个数据，输出时又该如何写入呢，写入之后又如何能够合并成一个可以识别的输出文件呢？分布式文件系统就是一个很好的解决方案。（这就是HDFS的由来）

其次是数据计算。前面说过，我们要让数据所在的服务器自行读取文件在本地的数据块，并进行计算。首先是本地读取完数据块后，执行的初步计算并得出结果；然而问题来了，在所有本地服务计算完成后，他们的计算结果中一般都存在维度重叠的问题（即服务器1计算结果中有A B C三个维度统计数据，而服务器2中有A C D E4个维度的数据，此时不能直接将各服务器的结果写入输出文件），因此还必须将这些计算结果进一步合并，以保证每个维度的key是唯一的。因此计算过程应该有两部分任务组成：一是本地服务器计算统计后得出初步结果（也叫中间数据）；二是对这些中间数据进行最终合并计算。Hadoop的大部分计算框架基本都是这两部走的，只是具体执行方式不太一样罢了（比如spark会把中间数据放在内存中而不是HDFS从而提高运行速度等）





























